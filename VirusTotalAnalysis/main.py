"""
Stupidly parallel binary analysis with VirusTotal.
"""

from argparse import ArgumentParser
import json
import multiprocessing as mp
import os
from pathlib import Path
from pprint import pprint
from random import shuffle
from statistics import mean, median, stdev
import sys
import time
from typing import Any, Optional

from tqdm import tqdm
from vt import Client

from cfg import KEY


def chunk_list(input_list, n):
    """Split a list into N roughly equal-sized chunks."""
    # Calculate the approximate chunk size
    chunk_size = len(input_list) // n

    # Initialize the starting and ending indices for each chunk
    start = 0
    end = chunk_size

    # Create and store each chunk in a list
    chunks = []
    for _ in range(n):
        if end > len(input_list):
            end = len(input_list)
        chunks.append(input_list[start:end])
        start = end
        end += chunk_size

    return chunks


def file_of_files_to_files(file: Path) -> list[Path]:
    files = [Path(f) for f in file.read_text().split("\n")]
    idx = set()
    for i, f in enumerate(files):
        if f.name in ("", "\n", " ", "\t"):
            idx.add(i)
        elif not f.exists():
            print(f"FileNotFound: {f.as_posix()}", file=sys.stderr)
    return [f for i, f in enumerate(files) if i not in idx]


def get_vt_analysis(file: Path, client: Client) -> Any:
    with open(file, "rb") as fp:
        return client.scan_file(fp, wait_for_completion=True)


def main(files: list[Path], output: Path, errors: Path) -> None:
    output.mkdir(exist_ok=True, parents=True)
    errors.touch()
    files.sort(key=lambda p: p.stat().st_size)

    times = []
    with Client(KEY) as client:
        for f in tqdm(files):
            s = time.time()

            try:
                a = get_vt_analysis(f, client)
            except Exception as e:
                with open(errors, "a") as fp:
                    fp.write(f"{f.as_posix()},{str(e)}")
                continue

            if not hasattr(a, "results"):
                with open(errors, "a") as fp:
                    fp.write(f"{f.as_posix()},None")
                continue

            times.append(time.time() - s)
            r = a.results
            with open(output / (f.name + ".report"), "w") as fp:
                json.dump(r, fp, indent=4)

    success = round(100 * len(times) / len(files))
    print(f"{success=}%")
    print(f"{mean(times)=}")
    print(f"{median(times)=}")
    print(f"{stdev(times)=}")


def prep_for_stupid_parallelism(files: list[Path], num_workers: int) -> None:
    shuffle(files)
    chunks = chunk_list(files, num_workers)
    for i in range(num_workers):
        with open(f"file_of_files/{i}.txt", "w") as fp:
            for f in chunks[i]:
                fp.write(f.as_posix() + "\n")

    print("Done with prep. Enter the following commands.")
    for i in range(num_workers):
        print(
            f"python main.py --file_of_files=file_of_files/{i}.txt --output=output/{i} --errors=errors/{i}.txt"
        )


def cli() -> None:
    parser = ArgumentParser()
    parser.add_argument("--file", type=Path)
    parser.add_argument("--file_of_files", type=Path)
    parser.add_argument("--directory", type=Path)
    parser.add_argument("--output", type=Path)
    parser.add_argument("--errors", type=Path)
    parser.add_argument("--prep", action="store_true")
    parser.add_argument("--num_workers", type=int)
    args = parser.parse_args()

    if args.file:
        files = [args.file]
    elif args.file_of_files:
        files = file_of_files_to_files(args.file_of_files)
    elif args.directory:
        files = list(args.directory.iterdir())
    else:
        raise ValueError("No files to scan")

    if args.prep:
        prep_for_stupid_parallelism(files, args.num_workers)
    else:
        main(files, args.output, args.errors)


# def debug() -> None:
#    files = list(Path("/home/lk3591/Documents/datasets/Windows/processed/train").iterdir())
#    prep_for_stupid_parallelism(files, 16)
#    main(files, Path("./output"), Path("./errors.txt"))


if __name__ == "__main__":
    cli()
